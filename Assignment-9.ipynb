{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1f997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of artificial intelligence concerned with the interactions between computers and human (natural) languages.\n",
      "It involves tasks such as language translation, sentiment analysis, and speech recognition.\n",
      "\n",
      "\n",
      "Tokens after Tokenization:\n",
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'involves', 'tasks', 'language', 'translation', 'sentiment', 'analysis', 'speech']\n",
      "\n",
      "Stemmed Tokens:\n",
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'involv', 'task', 'languag', 'translat', 'sentiment', 'analysi', 'speech']\n",
      "\n",
      "Lemmatized Tokens:\n",
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'natural', 'language', 'involves', 'task', 'language', 'translation', 'sentiment', 'analysis', 'speech']\n",
      "\n",
      "Tokens using Gensim Simple Preprocess:\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'subfield', 'of', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'it']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample text (since we don't have the file)\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of artificial intelligence concerned with the interactions between computers and human (natural) languages.\n",
    "It involves tasks such as language translation, sentiment analysis, and speech recognition.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Lowercase and remove punctuation\n",
    "tokens = [word.lower() for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# Remove stopwords\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Using Gensim's simple_preprocess function to preprocess the text (lowercasing, tokenization)\n",
    "gensim_tokens = simple_preprocess(text)\n",
    "\n",
    "# Display results\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nTokens after Tokenization:\")\n",
    "print(tokens[:20])  # Display first 20 tokens\n",
    "print(\"\\nStemmed Tokens:\")\n",
    "print(stemmed_tokens[:20])  # Display first 20 stemmed tokens\n",
    "print(\"\\nLemmatized Tokens:\")\n",
    "print(lemmatized_tokens[:20])  # Display first 20 lemmatized tokens\n",
    "print(\"\\nTokens using Gensim Simple Preprocess:\")\n",
    "print(gensim_tokens[:20])  # Display first 20 tokens from Gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1a29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
